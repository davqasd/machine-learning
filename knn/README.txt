ВАЖНО! Ты можешь вводить в консоли питоновской любые команды. Набери в терминале команду `python` и скопируй это построчно:
```
stat = [0 for i in range(2)]
stat
```

ВАЖНЫЙ И САМЫЙ ГЛАВНЫЙ МОМЕНТ: везде надо соблюдать порядок индексов, то есть во всех данных первый элемент - это категория, второй - это данные по оси X, третий - это данные по оси Y.


 !!! Parser !!!
1. Метод `__init__(self, filename)` вызывается каждый раз при инициализации парсера, то есть
`Parser('123.txt')` - создаст объект класса Parser и сразу вызовет метод `__init__`
2. В методе `__init__` мы открыли файл и прочитали его построчно (`f.readlines()`) при этом разделяя данные через пробел (`delimiter = ' '`)
Например
У нас есть файл `1.data`:
```
1 2 5 7 1
```
Выполним следующее
```
f = open('1.data', "r")
list(csv.reader(f.readlines(), delimiter = ' '))
```
Получим
```
[1, 2, 5, 7, 1]
```
3. Во все методы класса нужно первым параметром передавать `self`. Это какой-то
МЕГА питонинсткий костыль, который ни в одном ЯП не встречал, но он нужен...
И ещё один МЕГА костыль - это определять функцию в функции. Вообще пиздец - нигде
такой херни не видел, а здесь норма и позволяет создавать так область видимости или выходить из двойного цикла... В общем не удивляйся этим двум костылям питонинстким.
4. `get_column(self, n)` - просто чтобы получить строку в массив, то есть
`for row in self.parsed_file:` - идём по каждой строке нашего `self.parsed_file`
Далее `col.append(row[n - 1])` - в массив col добавляем столбец под номером n - 1.
минус 1, потому что в файле `horse-colic.names` данные нумеруются с 1, а массивы с 0. Просто для удобства работы.
5. `create_data_with_filter(self, array_indexes):` - не пугайся, он всего лишь берёт из `self.parsed_file` данные в нужном виде. То есть, например, если вызвать этот метод так:
```
parser = Parser("horse-colic.data")
parser.create_data_with_filter([1, 16, 20])
```
То этот метод возьмёт и запишет в self.data_rows - построчно столбцы 1, 16 и 20, то есть
Вот наши 4 строки из файла:
```
2 1 530101 38.50 66 28 3 3 ? 2 5 4 4 ? ? ? 3 5 45.00 8.40 ? ? 2 2 11300 00000 00000 2
1 1 534817 39.2 88 20 ? ? 4 1 3 4 2 ? ? ? 4 2 50 85 2 2 3 2 02208 00000 00000 2 
2 1 530334 38.30 40 24 1 1 3 1 3 3 1 ? ? ? 1 1 33.00 6.70 ? ? 1 2 00000 00000 00000 1
1 9 5290409 39.10 164 84 4 1 6 2 2 4 4 1 2 5.00 3 ? 48.00 7.20 3 5.30 2 1 02208 00000 00000 1 
```
В `self.data_rows` будет:
```
[[1.0, 5.0, 7.2], [1.0, 3.0, 6.0], [2.0, 7.2, 6.1], [1.0, 4.5, 6.8]]
```
а в `self.data_columns` будет:
```
[[1.0, 1.0, 2.0, 1.0], [5.0, 3.0, 7.2, 4.5], [7.2, 6.0, 6.1, 6.8]]
```
Просто для удобства решил распарсить данные и сразу их записать в строку и в столбец.

Кроме того на каждой итерации очищаю данные от ?. То есть, если хоть в каком-то
выбранном значении из выбранных столбцов будет ?, то строка сразу отбрасывается:
`return 'None'` и так как это обёрнуто в функцию `doWork`, то 
```
vals = doWork()
if vals == 'None':
  continue
```
благодаря этому мы выходим из текущей строки и ничего в `self.data_rows` и
`self.data_columns` не добавляем.


 !!! Далее наши методы для `MyMachineLearning`.  !!!
1. В init мы парсим и находим 
a) тренировочные данные `self.train_data_rows`
б) тестовые данные `self.test_data_rows`
в) те же тестовые данные, но с выборанной категорией, чтобы можно было оценить эффективность алгоритма `self.test_data_with_category_rows`
ВАЖНЫЙ И САМЫЙ ГЛАВНЫЙ МОМЕНТ: везде надо соблюдать порядок индексов, то есть во всех данных 1 - это категория, 2 - это данные по оси X, 3 - это данные по оси Y.
Например выберем тестовые данные `self.train_data_parser.create_data_with_filter([category_n, n1, n2])`. Здесь номера столбцов такие, как я описал и данные вернутся точно так же, например
`self.train_data_rows` - это массив строк, где
```
[[категория, X, Y], [...], ...]
```
2. `show_train_data` - здесь мы тупо отображаем данные. Как видишь, я сам сгенерил цвета, их можно менять, причём `c=train_data[0]` - это наши категории (см. ВАЖНЫЙ И САМЫЙ ГЛАВНЫЙ МОМЕНТ). Если смотреть на выборку, то наши данные имеют номера 1 и 2. Это и используется для индексов для выбора цвета из массива, поэтому есть `cmap=classColormap` - который берёт индекс из `c=train_data[0]`, подставляет в массив `classColormap` и мы получаем для каждой точки свой цвет.
`train_data[1]` - это значения по X, это по Y - `train_data[2]`.
3. `classifyKNN (self, trainData, testData, k):` - метод, ради чего и затевалась вся эта пьянка.
а) Ищем количество классов в выборке `self.count_uniq_first_column(trainData)`
б) функция вычисления расстояния между двумя точками `dist (a, b)`
в) цикл по всем точкам тестовой выборки `testPoint in testData:`
В нём мы делаем вот такую магию.
```
testDist = [ [dist(testPoint, [trainData[i][1], trainData[i][2]]), trainData[i][0]] for i in range(len(trainData))]
```
Не пугайся, это просто питонинский синтаксис задания массива через цикл.
В нём мы количество тренировочных точек раз (`for i in range(len(trainData))`) выполняем следующее: 
находим массив dist(testPoint, [trainData[i][1], trainData[i][2]]) - вычисляем для тестовой точки все расстояния до тренировочных точек. `trainData[i][0]` - это категория (см. ВАЖНЫЙ И САМЫЙ ГЛАВНЫЙ МОМЕНТ).
Следовательно в testDist мы имеем массив из всех расстояний ОДНОЙ тестовой точки до ВСЕХ тренировочных точек и вторым элементом идёт категория тренировочной точки. 
Пример:
У нас есть большая тренировочная выборка и одна тестовая точка.
По алгоритму в `testDist` запишется
```
[
[расстояние тестовой точки до тренировочной точки 1, категория 1],
[расстояние тестовой точки до тренировочной точки 2, категория 2],
[расстояние тестовой точки до тренировочной точки 3, категория 1],
...
[расстояние тестовой точки до тренировочной точки n, категория 1]
]
```
Категория 1 и 2, потому что у нас для 28 столбца всего 2 категории.

Далее
```
stat = [0 for i in range(numberOfClasses)]
```
Это просто массив нулей от 0 до numberOfClasses раз, то есть если numberOfClasses = 2, то
```
>>> stat = [0 for i in range(2)]
>>> stat
[0, 0]
```

`for d in sorted(testDist)[0:k]:` - сортируем данные и оставляем первые k элементов - это важно, в этом смысл алгоритма. То есть мы оставляем только k ближайших тренировочных точек к выбранной тестовой 

`stat[int(d[1]) - 1] += 1` - здесь для каждого типа находится сумма, то есть, например для этого примера
```
[
[расстояние тестовой точки до тренировочной точки 1, категория 1],
[расстояние тестовой точки до тренировочной точки 2, категория 2],
[расстояние тестовой точки до тренировочной точки 3, категория 1],
[расстояние тестовой точки до тренировочной точки n, категория 1]
]
```
мы получим массив stat = [3, 1], то есть у нас 3 точки принадлежат к 1 категории (нулевой индекс - первая категория, потому что минусую единицу) и 1 точка ко второй категории.

Дальше просто находим через сортировку для `testLabels` к какой категории больше подходит точка.

В конце возвращается просто массив категорий. Сопоставить с тестовой выборкой очень просто. Индекс каждого элемента тестовой выборки равен индексу полученного массива категорий.

4. calculateAccuracy (self, k) - здесь находится kNN, то есть массив категорий. Далее мы просто получаем массив категорий из `self.test_data_with_category_rows` - из тех же тестовых данных для которых был получен kNN, но с категориями.
Далее 
```
sum([int(testDataLabels[i]==testDataWithLabels[i][0]) for i in range(len(testDataWithLabels))]) / float(len(testDataWithLabels))
```
Для каждого значения тестовой выборки и kNN мы сравниваем значение. Если они равны, то 1, иначе 0. Дальше каждое такое число складываем, получаем сумму и делим на общее количество. Поэтому мы получаем точность в пределах от 0 до 1.

5. `loo_find_k(self, delta):`
Тут я просто по нарастающей определяю k смотря на точность при помощи `self.calculateAccuracy(k)`. Если полученная точность уже выше той, которую мы хотели получить, то мы нашли нужный нам k.

6. `showDataOnMesh (self, k):` - это отрисовка последнего рисунка.
`generateTestMesh` - задание окна сетки и h - шаг.
Далее рисуем сетку, где для каждой точки применяется наш kNN, чтобы можно было отрисовать красивую сетку. Делается через 
```
testMeshLabels = self.classifyKNN (trainData, zip(testMesh[0].ravel(), testMesh[1].ravel()), k)
```


 !!! Сам запуск программы  !!!
```
parser = Parser("horse-colic.data")
parser.create_data_with_filter([1, 16, 20])
print parser.data_columns
print parser.data_rows
```
Просто показываем, что мы не абы что распарсили

```
categories = [1, 2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 21, 23, 24, 25, 26, 27, 28]
non_cat = [3, 4, 5, 6, 16, 19, 20, 22]
```
Это просто для справочной информации где категориальные фичи, а где нет.

```
machine = MyMachineLearning(28, 20, 22)
```
Оу ееес, запуск дур-машины для столбцов 28, 20, 22 (см. ВАЖНЫЙ И САМЫЙ ГЛАВНЫЙ МОМЕНТ)

Находим наш k через LOO
```
k = machine.loo_find_k(0.000000001)
```

Вычисляем точность
```
k = machine.loo_find_k(0.000000001)
```

Показываем наши тренировочные данные
```
machine.show_train_data()
```

Показываем kNN в действии
```
print machine.classifyKNN(machine.train_data_rows, machine.test_data_rows, k)
```

Показываем наше великолепие. kNN в действии!
```
machine.showDataOnMesh(k)
```
